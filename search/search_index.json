{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Crank-ML","text":"<p>A selection of single-file machine learning recipes for <code>pytorch</code>. The provided implementations are intended to be self-contained and reusable in different contexts and problems. The goals of this project is:</p> <ul> <li>Single-file implementation. Each component and single script only depends on <code>pytorch</code> and is placed in a single file. As such there may be duplicated code in places, but this is intentional. </li> <li>Provide recipes via examples for machine learning use cases in a manner that is comparable to <code>scikit-learn</code>. </li> </ul>"},{"location":"#design-principles","title":"Design Principles","text":"<ul> <li>the only dependencies should be pytorch - no other preprocessing library is required when performing inference workflows. All items can be exported to <code>onnx</code> by default</li> <li>API naming should follow the broad patterns established in <code>scikit-learn</code>, however the actual interfaces will remain as <code>pytorch</code>-style inputs/outputs</li> <li>To support best practises, we will encourage using lightning</li> <li>Parameters which are not differentiable are updated stochastically via polyak averaging (e.g. <code>KBinsDiscretizer</code>)</li> </ul>"},{"location":"#implementations","title":"Implementations","text":"Implementation Description SGD <code>linear_model/sgd_classifier.py</code>, <code>linear_model/sgd_regressor.py</code> KBinsDiscretizer <code>preprocessing/kbins_discretizer.py</code> StandardScaler <code>preprocessing/standard_scaler.py</code> FactorizationMachine <code>factorization_machine/factorization_machine_classifier.py</code> FieldawareFactorizationMachine <code>factorization_machine/fieldaware_factorization_machine_classifier.py</code> this variation uses random n latent variables NeuralDecisionForest <code>tree/neural_decision_forest_classifier.py</code> this variation uses smoothstep instead of logistic function for the soft routing. See: https://arxiv.org/abs/2002.07772 NeuralDecisionBoosting <code>tree/neural_decision_boosting_classifier.py</code> this neural decision forest with gentleboost for the boosting variation KMeans <code>cluster/kmeans.py</code> this is not a differentiable variation PCA <code>decomposition/pca.py</code> TabNet <code>tabnet/tabnet_classifier.py</code> tabnet implementation without the pre-training step, based on the dreamquark-ai implementation but now ONNX exportable TabNet <code>tabnet/tabnet_regressor.py</code> tabnet implementation without the pre-training step, based on the dreamquark-ai implementation but now ONNX exportable TabNetPretraining <code>impute/tabnet_pretraining</code> tabnet pretraining for imputation using encoder/decoder architecture"},{"location":"api/cluster/kmeans/","title":"KMeans","text":"<p><code>crank_ml.cluster.kmeans.KMeans</code></p> <p>The most common way to implement batch k-means is to use Lloyd's algorithm, which consists in assigning all the data points to a set of cluster centers and then moving the centers accordingly.</p> <p>In this implementation we start by finding the cluster that is closest to the current observation. We then move the cluster's central position towards the new observation. The halflife parameter determines by how much to move the cluster toward the new observation.</p> <p>The KMeans implementation does not require learning via differentiation, and is updated analytically.</p>"},{"location":"api/cluster/kmeans/#parameters","title":"Parameters","text":"Parameter Description <code>n_features</code> Number of input features <code>n_clusters</code> (Default: <code>8</code>) Maximum number of clusters to assign. <code>halflife</code> (Default: <code>0.5</code>) Amount by which to move the cluster centers, a reasonable value if between 0 and 1"},{"location":"api/cluster/kmeans/#example","title":"Example","text":"<pre><code>import numpy as np\nimport torch\nfrom crank_ml.cluster.kmeans import KMeans\nX = torch.from_numpy(np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])).float()\nkmeans = KMeans(n_clusters=2, n_features=2)\nfor _ in range(5):\n_ = kmeans(X)\nkmeans.eval()\nlabels = kmeans(X)\n</code></pre>"},{"location":"api/decomposition/pca/","title":"PCA","text":"<p><code>crank_ml.decomposition.pca.PCA</code></p> <p>Performs linear Principal Component Analysis (PCA) on a low-rank matrix, batches of such matrices, or sparse matrix. This approach uses the nearly optimal approximation of a singular value decomposition of a centered matrix.</p> <p>Our implementation is a wrapper over the <code>pytorch</code> implementation with polyak averaging over the weights. As a consequence this may not be stable. </p>"},{"location":"api/decomposition/pca/#parameters","title":"Parameters","text":"Parameter Description <code>n_features</code> Number of input features <code>n_components</code> (Default: <code>6</code>) Maximum number of components to keep. <code>polyak_weight</code> (Default: <code>0.1</code>) The weight update rate, a reasonable value if between 0 and 1"},{"location":"api/decomposition/pca/#example","title":"Example","text":"<pre><code>import numpy as np\nimport torch\nfrom crank_ml.decomposition.pca import PCA\nX = torch.from_numpy(np.array([[1, 2, 3], [1, 4, 5], [1, 0, 1], [10, 2, 2], [10, 4, 3], [10, 0, 1]])).float()\npca = PCA(n_features=3, n_components=2)\nfor _ in range(5):\n_ = pca(X)\npca.eval()\npca_decomposition = pca(X)\n</code></pre>"},{"location":"api/factorization_machine/factorization_machine_classifier/","title":"FactorizationMachineClassifier","text":"<p><code>crank_ml.factorization_machine.factorization_machine_classifier.FactorizationMachineClassifier</code></p> <p>Factorization Machine for classification.</p> \\[ \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\] <p>Where \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors respectively. </p>"},{"location":"api/factorization_machine/factorization_machine_classifier/#parameters","title":"Parameters","text":"Parameter Description <code>n_features</code> Number of input features <code>embed_dim</code> (Default: <code>64</code>) Embedding dimension for the latent variables <code>n_classes</code> (Default: <code>2</code>) The number of classes in the classification problem <code>penalty</code> (Default: <code>l2</code>) The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None``. <code>alpha</code> (Default: <code>0.0001</code>) Constant that multiplies the regularization term. The higher the value, the stronger the regularization. <code>l1_ratio</code> (Default: <code>0.15</code>) The Elastic Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. <code>l1_ratio=0</code> corresponds to L2 penalty, <code>l1_ratio=1</code> to L1. Only used if penalty is 'elasticnet'."},{"location":"api/factorization_machine/factorization_machine_classifier/#example","title":"Example","text":"<pre><code>import lightning.pytorch as pl\nimport torch\nfrom sklearn.datasets import make_moons\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom crank_ml.factorization_machine.factorization_machine_classifier import FactorizationMachineClassifier\nclass FMClassifier(pl.LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.model = FactorizationMachineClassifier(2, n_classes=2)\ndef forward(self, X):\nreturn self.model(X).float()\ndef training_step(self, batch, batch_idx):\nloss, acc = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"train_acc\": acc, \"train_loss\": loss}\nself.log_dict(metrics)\nloss = loss + self.model.penalty()\nreturn loss\ndef validation_step(self, batch, batch_idx):\nloss, acc = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"val_acc\": acc, \"val_loss\": loss}\nself.log_dict(metrics)\nreturn metrics\ndef test_step(self, batch, batch_idx):\nloss, acc = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"test_acc\": acc, \"test_loss\": loss}\nself.log_dict(metrics)\nreturn metrics\ndef _shared_eval_step(self, batch, batch_idx):\nx, y = batch\ny_hat = self.forward(x)\nloss_fn = torch.nn.BCELoss()\nloss = loss_fn(y_hat, y)\nacc = torch.mean((torch.round(y_hat) == y).float())\nreturn loss, acc\ndef configure_optimizers(self):\nreturn torch.optim.Adam(self.parameters(), lr=0.02)\nn_samples = 1000\nX, y = make_moons(n_samples=n_samples, noise=0.2, random_state=0)\nX_train_, X_test, y_train_, y_test = train_test_split(X, y.reshape(-1, 1), test_size=0.33, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, test_size=0.33, random_state=42)\ntrain_data = DataLoader(\nTensorDataset(torch.from_numpy(X_train).float(), torch.from_numpy(y_train).float()),\nbatch_size=64,\n)\ntest_data = DataLoader(\nTensorDataset(torch.from_numpy(X_test).float(), torch.from_numpy(y_test).float()),\nbatch_size=64,\n)\nval_data = DataLoader(\nTensorDataset(torch.from_numpy(X_val).float(), torch.from_numpy(y_val).float()),\nbatch_size=64,\n)\ntrainer = pl.Trainer(accelerator=\"cpu\", max_epochs=100)\nmodel = FMClassifier()\ntrainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\ntrainer.test(dataloaders=train_data)\ntrainer.test(dataloaders=val_data)\ntrainer.test(dataloaders=test_data)\n</code></pre>"},{"location":"api/factorization_machine/factorization_machine_regressor/","title":"FactorizationMachineRegressor","text":"<p><code>crank_ml.factorization_machine.factorization_machine_regressor.FactorizationMachineRegressor</code></p> <p>Factorization Machine for regression.</p> \\[ \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_j, \\mathbf{v}_{j'} \\rangle x_{j} x_{j'} \\] <p>Where \\(\\mathbf{v}_j\\) and \\(\\mathbf{v}_{j'}\\) are \\(j\\) and \\(j'\\) latent vectors respectively. </p>"},{"location":"api/factorization_machine/factorization_machine_regressor/#parameters","title":"Parameters","text":"Parameter Description <code>n_features</code> Number of input features <code>embed_dim</code> (Default: <code>64</code>) Embedding dimension for the latent variables <code>penalty</code> (Default: <code>l2</code>) The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None``. <code>alpha</code> (Default: <code>0.0001</code>) Constant that multiplies the regularization term. The higher the value, the stronger the regularization. <code>l1_ratio</code> (Default: <code>0.15</code>) The Elastic Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. <code>l1_ratio=0</code> corresponds to L2 penalty, <code>l1_ratio=1</code> to L1. Only used if penalty is 'elasticnet'."},{"location":"api/factorization_machine/fieldaware_factorization_machine_classifier/","title":"FieldawareFactorizationMachineClassifier","text":"<p><code>crank_ml.factorization_machine.fieldaware_factorization_machine_classifier.FieldawareFactorizationMachineClassifier</code></p> <p>Fieldaware Factorization Machine for classification.</p> \\[ \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'} \\] <p>Where \\(\\mathbf{v}_{j, f_{j'}}\\) is the latent vector corresponding to \\(j\\) feature for \\(f_{j'}\\) field, and \\(\\mathbf{v}_{j', f_j}\\) is the latent vector corresponding to \\(j'\\) feature for \\(f_j\\) field. </p>"},{"location":"api/factorization_machine/fieldaware_factorization_machine_classifier/#parameters","title":"Parameters","text":"Parameter Description <code>n_features</code> Number of input features <code>embed_dim</code> (Default: <code>64</code>) Embedding dimension for the latent variables <code>n_latent_factors</code> (Default: <code>10</code>) The number of latent factors <code>n_classes</code> (Default: <code>2</code>) The number of classes in the classification problem <code>penalty</code> (Default: <code>l2</code>) The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None``. <code>alpha</code> (Default: <code>0.0001</code>) Constant that multiplies the regularization term. The higher the value, the stronger the regularization. <code>l1_ratio</code> (Default: <code>0.15</code>) The Elastic Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. <code>l1_ratio=0</code> corresponds to L2 penalty, <code>l1_ratio=1</code> to L1. Only used if penalty is 'elasticnet'."},{"location":"api/factorization_machine/fieldaware_factorization_machine_regressor/","title":"FieldawareFactorizationMachineRegressor","text":"<p><code>crank_ml.factorization_machine.fieldaware_factorization_machine_regressor.FieldawareFactorizationMachineRegressor</code></p> <p>Fieldaware Factorization Machine for regression.</p> \\[ \\hat{y}(x) = w_{0} + \\sum_{j=1}^{p} w_{j} x_{j}  + \\sum_{j=1}^{p} \\sum_{j'=j+1}^{p} \\langle \\mathbf{v}_{j, f_{j'}}, \\mathbf{v}_{j', f_j} \\rangle x_{j} x_{j'} \\] <p>Where \\(\\mathbf{v}_{j, f_{j'}}\\) is the latent vector corresponding to \\(j\\) feature for \\(f_{j'}\\) field, and \\(\\mathbf{v}_{j', f_j}\\) is the latent vector corresponding to \\(j'\\) feature for \\(f_j\\) field. </p>"},{"location":"api/factorization_machine/fieldaware_factorization_machine_regressor/#parameters","title":"Parameters","text":"Parameter Description <code>n_features</code> Number of input features <code>embed_dim</code> (Default: <code>64</code>) Embedding dimension for the latent variables <code>n_latent_factors</code> (Default: <code>10</code>) The number of latent factors <code>penalty</code> (Default: <code>l2</code>) The penalty (aka regularization term) to be used. Defaults to 'l2' which is the standard regularizer for linear SVM models. 'l1' and 'elasticnet' might bring sparsity to the model (feature selection) not achievable with 'l2'. No penalty is added when set to `None``. <code>alpha</code> (Default: <code>0.0001</code>) Constant that multiplies the regularization term. The higher the value, the stronger the regularization. <code>l1_ratio</code> (Default: <code>0.15</code>) The Elastic Net mixing parameter, with <code>0 &lt;= l1_ratio &lt;= 1</code>. <code>l1_ratio=0</code> corresponds to L2 penalty, <code>l1_ratio=1</code> to L1. Only used if penalty is 'elasticnet'."},{"location":"api/impute/tabnet_pretraining/","title":"TabNetPretraining","text":"<p><code>crank_ml.impute.tabnet_pretraining.TabNetPretraining</code></p> <p>This implements the unsupervised TabNet encoder-decoder approach to imputing missing values in datasets.</p>"},{"location":"api/impute/tabnet_pretraining/#example","title":"Example","text":"<p>Notice that in the example below, the label is not used in the loss function or the training step.</p> <pre><code>import lightning.pytorch as pl\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom crank_ml.impute.tabnet_pretraining import TabNetPretraining\nclass TabNetImputer(pl.LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.model = TabNetPretraining(4)\ndef forward(self, X):\nreturn self.model(X)\ndef training_step(self, batch, batch_idx):\nloss = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"train_loss\": loss}\nself.log_dict(metrics, on_epoch=True, prog_bar=True)\nreturn loss\ndef validation_step(self, batch, batch_idx):\nloss = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"val_loss\": loss}\nself.log_dict(metrics)\nreturn metrics\ndef test_step(self, batch, batch_idx):\nloss = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"test_loss\": loss}\nself.log_dict(metrics)\nreturn metrics\ndef _shared_eval_step(self, batch, batch_idx):\nx, _ = batch\noutput, embedded_x, obf_vars = self.model(x)\nloss = self.model.compute_loss(output, embedded_x, obf_vars)\nreturn loss\ndef configure_optimizers(self):\nreturn torch.optim.Adam(self.parameters(), lr=0.02)\nX, y = load_iris(return_X_y=True)\nX_train_, X_test, y_train_, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, test_size=0.33, random_state=42)\ntrain_data = DataLoader(\nTensorDataset(torch.from_numpy(X_train).float(), F.one_hot(torch.from_numpy(y_train), num_classes=3).float()),\nbatch_size=64,\n)\ntest_data = DataLoader(\nTensorDataset(torch.from_numpy(X_test).float(), F.one_hot(torch.from_numpy(y_test), num_classes=3).float()),\nbatch_size=64,\n)\nval_data = DataLoader(\nTensorDataset(torch.from_numpy(X_val).float(), F.one_hot(torch.from_numpy(y_val), num_classes=3).float()),\nbatch_size=64,\n)\ntrainer = pl.Trainer(accelerator=\"cpu\", max_epochs=250)\nmodel = TabNetImputer()\ntrainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\ntrainer.test(dataloaders=train_data)\ntrainer.test(dataloaders=val_data)\ntrainer.test(dataloaders=test_data)\n# usage\nmodel.eval()\nreconstruction, _, _ = model(torch.from_numpy(X_train).float())\n</code></pre>"},{"location":"introduction/basic-concepts/","title":"Basic concepts","text":"<p>Here are some high level concepts to give you an idea on how Crank can be used. </p>"},{"location":"introduction/basic-concepts/#deep-learning-first","title":"Deep Learning First","text":"<p>The focus of Crank is to provide tools to enable tabular machine learning via mini-batch learning. In general, we presume we are learning over a large dataset, too large to fit in memory and will leverage deep learning techniques to ensure it is appropriate. </p> <p>The data processed presumes everything occurs in an online fashion, rather than the typical traditional approach where it is available as a whole batch and processed altogether at one time.</p>"},{"location":"introduction/basic-concepts/#single-file-per-algorithm","title":"Single File Per Algorithm","text":"<p>This repository is for pedagogy purposes, but may be useful for you! Each implementation is designed to be part of its own standalone file, and aims to fulfil vanilla pytorch as much as possible. This is to ensure there is minimal dependencies and maximum flexibility to alter and use these algorithms to suit user needs. Infact, we expect users to simple take individual files and adapt the models as required. </p>"},{"location":"introduction/basic-concepts/#input-data-types","title":"Input Data Types","text":"<p>As we're reliant on vanilla pytorch, the training data is purely numeric. This may be overly restrictive in the real-world, though hopefully in conjunction with typical preprocessing pipelines it will be possible to have a reasonable experience when combined with libraries like lightning.</p>"},{"location":"introduction/installation/","title":"Introduction","text":"<p><code>crank_ml</code> is designed to work with Python 3.8 and above. To install the latest development version from Github:</p> <pre><code># via https\npip install git+https://github.com/8bit-pixies/Crank-ML\n# via ssh\npip install git+ssh://git@github.com:8bit-pixies/Crank-ML.git\n</code></pre> <p>Feel free to open an issue on Github if you are having trouble. </p>"},{"location":"introduction/getting-started/kmeans/","title":"KMeans","text":"<p>The most common way to implement batch k-means is to use Lloyd's algorithm, which consists in assigning all the data points to a set of cluster centers and then moving the centers accordingly.</p> <p>In this implementation we start by finding the cluster that is closest to the current observation. We then move the cluster's central position towards the new observation. The halflife parameter determines by how much to move the cluster toward the new observation.</p> <p>The KMeans implementation does not require learning via differentiation, and is updated analytically.</p> <pre><code>import numpy as np\nimport torch\nfrom crank_ml.cluster.kmeans import KMeans\nX = torch.from_numpy(np.array([[1, 2], [1, 4], [1, 0], [10, 2], [10, 4], [10, 0]])).float()\nkmeans = KMeans(n_clusters=2, n_features=2)\n</code></pre> <p>KMeans is updated when using the forward pass. To stop training, the <code>eval</code> method is used. </p> <pre><code>kmeans.eval()\n# this is now deterministic\nlabels = kmeans(X)\n</code></pre>"},{"location":"introduction/getting-started/multi-class-classification/","title":"Multi-Class Classification","text":"<p>Classification is about predicting an outcome from a fixed list of classes. The prediction is a probability distribution that assigns a probability to each possible outcome.</p> <p>A labeled classification sample is made up of a bunch of features and a class. The class is a boolean in the case of binary classification. We'll use the iris dataset as an example.</p>"},{"location":"introduction/getting-started/multi-class-classification/#defining-a-lightningmodule","title":"Defining a LightningModule","text":"<p>To walk through this example we use lightning. First we define a <code>LightningModule</code> to enable the PyTorch <code>nn.Module</code> to operate</p> <pre><code>import lightning.pytorch as pl\nimport torch\nimport torch.nn.functional as F\nfrom sklearn.datasets import load_iris\nfrom sklearn.model_selection import train_test_split\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom torchmetrics.functional import accuracy\nfrom crank_ml.linear_model.sgd_classifier import SGDClassifier\nclass IrisClassifier(pl.LightningModule):\ndef __init__(self):\nsuper().__init__()\nself.model = SGDClassifier(4, n_classes=3)\ndef forward(self, X):\nreturn self.model(X)\ndef training_step(self, batch, batch_idx):\nloss, acc = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"train_acc\": acc, \"train_loss\": loss}\nself.log_dict(metrics)\nloss = loss + self.model.penalty()\nreturn loss\ndef validation_step(self, batch, batch_idx):\nloss, acc = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"val_acc\": acc, \"val_loss\": loss}\nself.log_dict(metrics)\nreturn metrics\ndef test_step(self, batch, batch_idx):\nloss, acc = self._shared_eval_step(batch, batch_idx)\nmetrics = {\"test_acc\": acc, \"test_loss\": loss}\nself.log_dict(metrics)\nreturn metrics\ndef _shared_eval_step(self, batch, batch_idx):\nx, y = batch\ny_hat = self.model(x)\nloss = F.cross_entropy(y_hat, y)\nacc = accuracy(F.one_hot(torch.argmax(y_hat, axis=1), num_classes=3), y, task=\"multiclass\", num_classes=3)\nreturn loss, acc\ndef configure_optimizers(self):\nreturn torch.optim.Adam(self.parameters(), lr=0.02)\n</code></pre>"},{"location":"introduction/getting-started/multi-class-classification/#defining-the-dataset","title":"Defining the dataset","text":"<p>We can make use of the <code>DataLoader</code> with the appropriate splits.</p> <pre><code>X, y = load_iris(return_X_y=True)\nX_train_, X_test, y_train_, y_test = train_test_split(X, y, test_size=0.33, random_state=42)\nX_train, X_val, y_train, y_val = train_test_split(X_train_, y_train_, test_size=0.33, random_state=42)\ntrain_data = DataLoader(\nTensorDataset(torch.from_numpy(X_train).float(), F.one_hot(torch.from_numpy(y_train), num_classes=3).float()),\nbatch_size=64,\n)\ntest_data = DataLoader(\nTensorDataset(torch.from_numpy(X_test).float(), F.one_hot(torch.from_numpy(y_test), num_classes=3).float()),\nbatch_size=64,\n)\nval_data = DataLoader(\nTensorDataset(torch.from_numpy(X_val).float(), F.one_hot(torch.from_numpy(y_val), num_classes=3).float()),\nbatch_size=64,\n)\n</code></pre>"},{"location":"introduction/getting-started/multi-class-classification/#train-the-model","title":"Train the model","text":"<p>The Lightning Trainer can be used which abstracts away all the engineering complexity</p> <pre><code>trainer = pl.Trainer(max_epochs=100)\nmodel = IrisClassifier()\ntrainer.fit(model, train_dataloaders=train_data, val_dataloaders=val_data)\ntrainer.test(dataloaders=train_data)\ntrainer.test(dataloaders=val_data)\ntrainer.test(dataloaders=test_data)\n</code></pre>"}]}